---
title: "Statistical Roadmap and Data Intro"
author: "Alan Hubbard"
date: "`r Sys.Date()`"
output: pdf_document
bibliography: refs.bib
editor_options: 
  chunk_output_type: console
---

## Learning Objectives
1. Introduce the data being used in workshp
2. Introduce the statistical roadmap on which the methods are based.

## The Data
The data come from the Effect of water quality, sanitation, hand washing, and
nutritional interventions on child development in rural
Bangladesh (WASH Benefits Bangladesh):
a cluster-randomised controlled trial (@luby2018effects). The study enrolled enrolled pregnant women in their first or second trimester
from rural villages of Gazipur, Kishoreganj, Mymensingh, and Tangail districts of central Bangladesh, with an average
of eight women per cluster. Groups of eight geographically adjacent clusters were block-randomised, using a random
number generator, into six intervention groups (all of which received weekly visits from a community health promoter
for the first 6 months and every 2 weeks for the next 18 months) and a double-sized control group (no intervention or
health promoter visit). The six intervention groups were: chlorinated drinking water; improved sanitation;
handwashing with soap; combined water, sanitation, and handwashing; improved nutrition through counselling and
provision of lipid-based nutrient supplements; and combined water, sanitation, handwashing, and nutrition. In the workshop, we concentrate child-growth (size for age) as the outcome.  This trial was registered with ClinicalTrials.gov, number NCT01590095.

```{r, echo=T, include=F}
library(tidyverse)
library(tlverse)
library(xtable)

## Read in data
dat <- read_csv("../washb_data/washb_data.csv") 
tibble(dat)
```

For the purposes of this workshop, we we start by treating the data as independent and identically distributed (i.i.d.) random draws from a very large target population.  We could, with available options,  account for the clustering of the data (within sampled geographic units), but avoiding these details for this workshop.

We have 2 variables and 1 variable set of interest.  Our outcome, $Y$, is *whz* -- weight For height Z-score, the treatment of interest, $A$, is *tr* -- randomized treatment group, and 
our adjustment variables, $W$, are *everything else*.  This results in our observed data structure as $i=1,...,n$ i.i.d. copies of $O_i = (W_i,A_i,Y_i)$. 

Modifications of our methods for biased samples, repeated measures, etc. are available. 

### The variables
```{r, echo=FALSE, include=F}
categ <- dat[,c(2,6,8,10,14:28)]
ncat <- dim(categ)[2]
vars.cat <- names(categ)
cont <- dat[,-c(2,3,6,8,10,14:28)]
ncont <- dim(cont)[2]
vars.cont <- names(cont)

# First do "continuous"" variables
var.out <- NULL
tab1.out <- NULL
hline <- c(-1,0)
for(i in 1:ncont) {
var.out <- c(var.out,vars.cont[i])
  x = as.numeric(unlist(cont[,i]))
  tab1.out <- rbind(tab1.out,c(mean(x,na.rm=T),sd(x,na.rm=T),NA,NA))
  hline=c(hline,i)
}

tab1.out <- data.frame(levels="",tab1.out)
names(tab1.out)<- c("levels","Mean","SD","Count","Row.Prop")
# Do categorical variables
cnter <- ncont
for(i in 1:ncat) {
  hline <- c(hline,cnter)
  x = unlist(categ[,i])
  tt <- table(x)
  levels <- names(tt)
  nt <- length(tt)
  tt <- as.vector(tt)
  out <- data.frame(levels=levels,Mean=rep(NA,nt),SD=rep(NA,nt),Count=tt,Row.Perc=tt/sum(tt))
names(out)<- c("levels","Mean","SD","Count","Row.Prop")
  var.out <- c(var.out,c(vars.cat[i],rep("",nrow(out)-1)))
  cnter <- cnter+nrow(out)
  tab1.out <- rbind(tab1.out,out)
}

times <- rep("base",nrow(tab1.out))

tab1.out <- data.frame(Variable=var.out,tab1.out)
```


```{r, echo=F, include=F}
# Change label from tabl
tab1=xtable::xtable(tab1.out, 
  caption = "Distribution of variables used in WASH-Benefits Study", label = "table_1", digits = 4,rownames=F,display=c("s","s","s","f","f","d","f"),align = "l|l|r|c|c||c|c|")
print(tab1,type="latex",file="../vignettes/table1.tex",caption.placement="top",include.rownames=F,hline.after=hline)

```
\include{table1}

The distribution of relevant variables is shown in Table \ref{table_1}.  The top
rows summarize continuous variables, the remainder are categorical.  The asset 
variables reflect the socio-economic status of the subjects.  Notice also the 
uniform distribution of the treatment groups (with twice as many controls); this 
is of course by design.

## The model
The reason you're taking this course, presumably, is because you want to estimate relevant parameters in realistic models.  

### The statistical Model

One can break up the distribution of the observed data as follows: $P(O)=P(W,A,Y)=P(W)P(A|W)P(Y|A,W)$. To estimate a parameter of interest, the researcher does not necessarily need to specify these whole distributions and conditional distributions. Each estimator requires certain parts of the distribution, for example some require estimates of $E(Y|A,W)=$ mean of $Y$ within subgroups $(A,W)$, or the regression of the outcome on the exposure and confounders. 
At this stage in the road map the researcher specifies what statistical model she will use to estimate $E(Y|A,W)$ or other elements of the probability distribution that are needed to estimate the parameter of interest. By “statistical model” here we mean any constraints on the model form that are imposed by knowledge about the process - known aspects of how the data were generated.  Typically, the true model is a very big model, with few constraints if any on the data-generating distribution, or a semi-parametric model. Thus, with few constraints on the data-generating distribution, and a potentially large number of covariates, data-adaptive, machine-learning approaches are the only practical option. The remainder of the course is how to do this as efficiently and robustly as possible, depending on the goal of the analysis.

### The Causal Model
The next step is to use a causal framework to formalize the experiment and thereby define the parameter of interest. Causal graphs are one useful tool to express what we know about the causal relations among variables that are relevant to the question under study (@pearl_causality_2000). An illustration shows a simple causal graph, specifically a directed acyclic graph or DAG, to depict the causal relations between variables with a simple example of a binary exposure A, a binary outcome Y, and one categorical confounding variable W. The DAG for these relations is depicted below. 

\includegraphics[width=4.0in]{CausalGraph.pdf}

The $U_W$, $U_A$, and $U_Y$ are the unmeasured exogenous background characteristics that influence the value of each variable.  Alternatively, the same causal relations among variables can be represented with a series of equations:
\begin{eqnarray*}
W &=& f_W(U_W) \\
A &=& f_A(W,U_A) \\
Y&=&f_Y(W,A,U_Y). 
\end{eqnarray*}

Here $f_W, f_A$ and $f_Y$ denote that each variable ($W, A$ and $Y$ respectively) is a function of its parents and unmeasured background characteristics, but there is no imposition of any particular functional constraints.  For this reason, they are called non-parametric structural equations. The DAG and this series of non-parametric structural equations represent the same information. 

## Parameter of Interest

The first hypothetical experiment we will consider is assigning exposure to the whole population and observing the outcome, and then assigning no exposure to the whole population and observing the outcome. On the non-parametric structural equations, this corresponds to a comparison of the outcome distribution in the population under two interventions: 1) A is set to 1 for all individuals, and 2) A is set to 0 for all individuals. These interventions imply two new non-parametric structural equation models with first being:
\begin{eqnarray*}
W &=& f_W(U_W) \\
A &=& 1 \\
Y(1)&=&f_Y(W,1,U_Y), 
\end{eqnarray*}
and second just replacing the intervention of $A=1$ to $A=0$,
\begin{eqnarray*}
W &=& f_W(U_W) \\
A &=& 0 \\
Y(0)&=&f_Y(W,0,U_Y).
\end{eqnarray*}

In these equations, A is no longer a function of W because we have intervened on the graph and set A to the values 1 and 0. The new symbols Y(1) and Y(0) indicate the outcome variable in our population if it were generated by the respective NPSEMs above; these are often called counterfactuals. The difference between the means of the outcome under these two interventions defines a parameter that is often called the “Average Treatment Effect (ATE), or
\begin{equation}
\label{ate}
ATE = E_X(Y(1)-Y(0)),
\end{equation}
### Identifiability
where $E_X$ is the mean under the theoretical full data: $X=(W,Y(1),Y(0))$.
Because we can never observe both $Y(0)$ (counterfactual when $A=0$) and $Y(1)$, we can not estimate \ref{ate} directly.  Thus, we have to make assumptions to estimate this quantity from $O \sim P_0$, or the data-generating distribution.  Fortunately, given our causal model shown in the graph above, we can, with a couple of more assumptions, estimate the ATE even from observational data.  First, the causal graph implies that $Y(a) \perp A$ for all $a \in \mathcal{A}$, which is the randomization assumption.  Outside of the graph, one also needs a non-interference assumption (say the $Y$'s are independent) and positivity ($0<P_0(A=a\mid W)< 1$ for all $a$ and $W$).  Given these assumptions, then one can re-write the ATE as a function of $P_0$, specifically
\begin{equation}
\label{estimand}
ATE = E_0(Y(1)=Y(0)) = E_0\left( E_0[Y \mid A=1,W]-E_0[Y \mid A=0,W] \right),
\end{equation}
or the difference in predicted values for each subject in population and then averaging over those subjects.  Thus, a parameter of a theoretical "full" data distribution can be represented as an estimand of the observed data-distribution.  Significantly, there is nothing about the representation in \ref{estimand} that requires parameteric assumptions, so that the regressions on the right hand side can be estimated freely with machine learning.  With different parameters, there will be potentially different identifiability assumptions and the resulting estimands can be functions of different components of $P_0$.  We discuss several more complex ones in this workshop later.

## Estimator

We will discuss more in the specific sections later, but the goals of the estimator should be, among sensible (asymptotically consistent, regular) estimators
1. They are asymptotically efficient in the statistical model of interest.
2. They can be constructed for finite sample performance improvements.

## Inference

The estimators we discuss are asymptotically linear, meaning that the difference in teh estimate $\Psi(P_n)$ and the true parameter ($\Psi(P_0)$) can be represented in first order by a i.i.d. sum:
\begin{equation}
\label{IC}
\Psi(P_n) - \Psi(P_0) = \frac{1}{n} IC(O_i; \nu)+op(1/\sqrt{n})
\end{equation}
where $IC(O_i; \nu)$ is a function of the data and possibly other parameters, $\nu$.  Thus, in the univariate case, one can derive a 95\% confidence interval as:
\begin{equation}
\label{CI}
\Psi(P^*_n) \pm 1.96 \sqrt{\frac{\hat{\sigma}^2}{n}}
\end{equation}

where $SE=\sqrt{\frac{\hat{\sigma}^2}{n}}$ and $\hat{\sigma}^2$ is the sample variance of the estimated IC's: $IC(O;\hat{\nu})$.  One can use the functional delta method to derive the influence curve if a parameter of interest that is written as a function of other asymptotically linear estimators. 


## References

